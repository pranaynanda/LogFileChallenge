{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Log Stream Handlers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/pranaynanda/LogFileChallenge/blob/master/Log_Stream_Handlers.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "Mqd8ni1YrV5q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set Project\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "x_2V0Sotc38w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "#project_id = '[your Cloud Platform project ID]'\n",
        "!gcloud config set project bookshelf-1860044"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hKaYpJ9PrdIa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Get Files\n",
        "\n",
        "This above part could be removed. I used Google Colaboratory which are basically Google hosted Jupyter notebooks. The code above authenticates my GCP console to the notebook. I could make the files publically available either to all users or all Google authenticated users which would eliminate the need to autheticate or set project. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The code below could also be removed if we directly provide the file names. I did not want to copy them in the temporary instance over and over again."
      ]
    },
    {
      "metadata": {
        "id": "qoa0RWtCc7K5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Download the file from a given Google Cloud Storage bucket.\n",
        "!gsutil cp gs://ihopeitworks/* ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9CKgWU-rf9y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import Packages"
      ]
    },
    {
      "metadata": {
        "id": "12DW4nOyZuhv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import logging\n",
        "import sys\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aeN7psp6rl75",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Open YAML File\n",
        "\n",
        "I used YAML format instead of JSON because it is easier to read and also because I previously have worked with YAML in Ansible and GCP.\n"
      ]
    },
    {
      "metadata": {
        "id": "qhMuk3YIZ68E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"yaml_file.yaml\", 'r') as yamlfile:\n",
        "    config = yaml.load(yamlfile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ygJnayvYrrLB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Regex\n",
        "\n",
        "I used my experience with Splunk to define regex. Splunk takes in each line in the log file and then passes it through a regex to get fields."
      ]
    },
    {
      "metadata": {
        "id": "BYTL35Cr8Rza",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "regex=\"^((\\d{4}-\\d{2}-\\d{2})\\s(\\d{2}:\\d{2}:\\d{2}\\.\\d+))\\s\\[PID (\\d+)\\]\\s\\[(\\d+)\\w{2}\\]\\s\\[UID (\\w+)\\]\\s\\[(\\w+)\\]\\s(\\/\\w+)(.*Error name: (\\w.+))?(.*)$\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eeIuWBUrrtlZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Open Stream and check conditions from YAML (Incomplete)\n",
        "\n",
        "We open the file as an input stream and start reading a line at a time. Then extract the regex matches in a line and append them to the empty lists. I created empty lists because then from there on I can either map them to numpy or a pandas dataframe to run analysis. \n",
        "\n",
        "This part of the code is currently very static. The part that I couldn't get through was processing all of this while data was streaming. I could understand  that there is a need to create event handlers which would be something very similar to creating a custom Ansible module. What I could not understand was to handle new rules for which the functions have not been written so I could write something like [this](https://docs.ansible.com/ansible/2.3/dev_guide/developing_modules_general.html):\n",
        "\n",
        "At one point I could only either stream the data and output log if the condition matches or analyze the data, no both at the same time.\n",
        "\n",
        "For example, from the example rules, I could write a rule that if the error is equal to the value set in the YAML file, then write a log line, but I could not figure out how to do it if it matches a given count in the specified time delta.\n",
        " \n",
        "If I process information outside the loop that reads the lines, then we are out of the stream and if I process it inside the loop then there is only one value in the loop at that moment. \n",
        "\n",
        "If I initialize the empty lists in the loop, the lists are again fresh empty everytime a line is read.\n",
        "\n",
        "The way this is taken care of in Ansible and GCP is that in the background there is a preset YAML file that defines the style from which the scripts consume data.\n",
        "\n",
        "I took a similar approach and created a YAML file that contains some sample parameters and read from there."
      ]
    },
    {
      "metadata": {
        "id": "3qIpa9hoaZLm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('out.log', 'r') as sys.stdin:  # Open the file as an input stream\n",
        "\n",
        "    # Initialize empty lists. I did not want to run the analysis in the loop while it loads the stream.\n",
        "    \n",
        "    timestamps = []\n",
        "    pid = []\n",
        "    response_time_in_ms = []\n",
        "    uid = []\n",
        "    log_level = []\n",
        "    url = []\n",
        "    error_message = []\n",
        "    error_type = []\n",
        "    message = []\n",
        "    \n",
        "    lines = sys.stdin.readlines()\n",
        "  \n",
        "    # print(lines) ## List of lines\n",
        "  \n",
        "    for line in lines:  # Start reading from lines as an input stream\n",
        "  \n",
        "        regex_read = re.findall(regex,line) # single element tuple as list\n",
        " \n",
        "        #   Extracting Date/Time isn't required but comes handy.\n",
        "  \n",
        "        #   date = \"\".join([match[1] for match in regex_read]) \n",
        "        # This was the previous code where each parameter contained all values as a list of strings\n",
        "    \n",
        "        #   time = \"\".join([match[2] for match in regex_read])\n",
        "\n",
        "        timestamps.append(\"\".join([match[0] for match in regex_read])) \n",
        "        # These values contain list of lists that have a single value.\n",
        "        # We convert them into a string and then append them to the list.\n",
        "        pid.append(\"\".join([match[3] for match in regex_read]))\n",
        "    \n",
        "        response_time_in_ms.append(\"\".join([match[4] for match in regex_read]))\n",
        "    \n",
        "        uid.append(\"\".join([match[5] for match in regex_read]))\n",
        "    \n",
        "        log_level.append(\"\".join([match[6] for match in regex_read]))\n",
        "     \n",
        "        url.append(\"\".join([match[7] for match in regex_read]))\n",
        "    \n",
        "        error_message.append(\"\".join([match[8] for match in regex_read]))\n",
        "      \n",
        "        error_type.append(\"\".join([match[9] for match in regex_read]))\n",
        "    \n",
        "        message.append(\"\".join([match[10] for match in regex_read]))\n",
        "      \n",
        "      #try/catch exceptions within the loop\n",
        "      \n",
        "        try:\n",
        "            if (config['log_level']['enabled_flag']) == True:                         \n",
        "              #Check if we even need to write a log if with conditions based on log_level\n",
        "                if (config['log_level']['count_thresh'] < 7 and\n",
        "                   (len(set(log_level[-config['log_level']['count_thresh']:]))) == 1):\n",
        "                       logging.warning('Watch out!')\n",
        "                ##At each line check if last n elements that are specified as \n",
        "                #config['log_level']['count_thresh'] are same \n",
        "                #and write a log if the condition matches\n",
        "                                                        \n",
        "                                                                                    \n",
        "        except:\n",
        "          pass\n",
        "  \n",
        "  # The above solution kinda works but is way too complex and inefficient. \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vLQdnd5xaFXA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "  # The other idea was to use dataframes\n",
        "  \n",
        "  \n",
        "#   logfile=pd.DataFrame({'Timestamp': timestamps,\n",
        "#                      'PID': pid,\n",
        "#                      'Response Time': response_time_in_ms,\n",
        "#                      'UID': uid,\n",
        "#                      'Log Level': log_level,\n",
        "#                      'URL': url,\n",
        "#                      'error_message': error_message,\n",
        "#                      'error_type': error_type,\n",
        "#                      'message': message})\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "#   logfile=logfile.replace('', np.nan)\n",
        "#   logfile = logfile.dropna(how='all') \n",
        "#   logfile['Timestamp'] = pd.to_datetime(logfile['Timestamp'])\n",
        "#   logfile['Response Time']=pd.to_numeric(logfile['Response Time'])\n",
        "  \n",
        "  \n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "#logfile.dtypes\n",
        "\n",
        "#print(logfile['Timestamp'].iloc[-1])\n",
        "\n",
        "#print (pd.date_range(start=logfile['Timestamp'].iloc[-1], end=logfile['Timestamp'].iloc[0],freq='S'))\n",
        "  \n",
        "  \n",
        "#   ############################################################################################\n",
        "#   #             INSERT TRY/CATCH STATEMENTS FOR THE CONDITIONS                               #\n",
        "#   ############################################################################################\n",
        "  \n",
        "  \n",
        "  \n",
        "# #print(type(configfg['log_level']['count_thresh']))\n",
        "\n",
        "\n",
        " \n",
        "#   #print(timestamps)\n",
        "\n",
        "    \n",
        "# #print(timestamps[0])\n",
        "        \n",
        "#print(timestamps)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}